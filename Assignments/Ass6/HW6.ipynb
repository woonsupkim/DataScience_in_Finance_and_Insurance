{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b03e27b",
   "metadata": {},
   "source": [
    "### Assignment 6\n",
    "### Prepared By: Woon Kim\n",
    "### UNI: wk2371\n",
    "### Course: ACTU 5841\n",
    "### Date: March 6 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07202f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9266356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9d626e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.189053</td>\n",
       "      <td>-0.522748</td>\n",
       "      <td>-0.413064</td>\n",
       "      <td>-2.441467</td>\n",
       "      <td>1.799707</td>\n",
       "      <td>-1.528474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.144166</td>\n",
       "      <td>-0.325423</td>\n",
       "      <td>0.773807</td>\n",
       "      <td>0.281211</td>\n",
       "      <td>-0.553823</td>\n",
       "      <td>1.246495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.977567</td>\n",
       "      <td>-0.310557</td>\n",
       "      <td>-0.328824</td>\n",
       "      <td>-0.792147</td>\n",
       "      <td>0.454958</td>\n",
       "      <td>-1.869208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.099198</td>\n",
       "      <td>0.545289</td>\n",
       "      <td>-0.607186</td>\n",
       "      <td>0.126828</td>\n",
       "      <td>-0.892274</td>\n",
       "      <td>-8.154580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.841465</td>\n",
       "      <td>0.188035</td>\n",
       "      <td>0.330571</td>\n",
       "      <td>0.410504</td>\n",
       "      <td>-1.010758</td>\n",
       "      <td>7.608370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>3.110155</td>\n",
       "      <td>-0.818143</td>\n",
       "      <td>1.255157</td>\n",
       "      <td>-0.149256</td>\n",
       "      <td>0.260780</td>\n",
       "      <td>11.115184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-0.546752</td>\n",
       "      <td>1.162408</td>\n",
       "      <td>-1.254861</td>\n",
       "      <td>-1.322278</td>\n",
       "      <td>-0.207827</td>\n",
       "      <td>-1.371915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-1.158040</td>\n",
       "      <td>0.703278</td>\n",
       "      <td>0.921895</td>\n",
       "      <td>-0.752996</td>\n",
       "      <td>0.783836</td>\n",
       "      <td>-0.347831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.662045</td>\n",
       "      <td>-0.044232</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>1.876453</td>\n",
       "      <td>-0.000428</td>\n",
       "      <td>3.607191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-0.911962</td>\n",
       "      <td>-0.917547</td>\n",
       "      <td>1.199997</td>\n",
       "      <td>-1.669013</td>\n",
       "      <td>-0.470244</td>\n",
       "      <td>-4.782707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5          y\n",
       "0    0.189053 -0.522748 -0.413064 -2.441467  1.799707  -1.528474\n",
       "1    1.144166 -0.325423  0.773807  0.281211 -0.553823   1.246495\n",
       "2    0.977567 -0.310557 -0.328824 -0.792147  0.454958  -1.869208\n",
       "3   -0.099198  0.545289 -0.607186  0.126828 -0.892274  -8.154580\n",
       "4    0.841465  0.188035  0.330571  0.410504 -1.010758   7.608370\n",
       "..        ...       ...       ...       ...       ...        ...\n",
       "195  3.110155 -0.818143  1.255157 -0.149256  0.260780  11.115184\n",
       "196 -0.546752  1.162408 -1.254861 -1.322278 -0.207827  -1.371915\n",
       "197 -1.158040  0.703278  0.921895 -0.752996  0.783836  -0.347831\n",
       "198 -0.662045 -0.044232  0.006761  1.876453 -0.000428   3.607191\n",
       "199 -0.911962 -0.917547  1.199997 -1.669013 -0.470244  -4.782707\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1791ab3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def center_normalize_df (dataframe):\n",
    "    \n",
    "    for column in dataframe:\n",
    "        dataframe[column] = dataframe[column] - np.mean(dataframe[column])\n",
    "        dataframe[column] = dataframe[column] / norm(dataframe[column])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def center_normalize_array (array):\n",
    "    array = array - np.mean(array)\n",
    "    array = array / norm(array)\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1102a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2_RMSE(y,X,B, y_hat):\n",
    "    \n",
    "    B = B.reshape(1,-1)\n",
    "    \n",
    "    y_avg = y_hat / len(y_hat)\n",
    "    \n",
    "    \n",
    "    if (y_hat.ndim == 2):\n",
    "        y_avg = y_hat[:,0] / len(y_hat[:,0])\n",
    "        SSR = np.sum((y-y_hat[:,0])**2)\n",
    "        SST = np.sum((y-y_avg)**2)\n",
    "\n",
    "        R2 = 1- SSR/SST\n",
    "        RMSE = np.sqrt(((y_hat[:,0] - y) ** 2).mean())\n",
    "        \n",
    "    else:\n",
    "        y_avg = y_hat / len(y_hat)\n",
    "        SSR = np.sum((y-y_hat)**2)\n",
    "        SST = np.sum((y-y_avg)**2)\n",
    "\n",
    "        R2 = 1- SSR/SST\n",
    "        RMSE = np.sqrt(((y_hat - y) ** 2).mean())\n",
    "\n",
    "    return RMSE, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a27b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_lin(B):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    reg = LinearRegression()\n",
    "    B = B.reshape(1,-1)\n",
    "    \n",
    "    kf_RMSE = []\n",
    "    kf_r2 = []\n",
    "    \n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "    \n",
    "    for train, test in cv.split(df):\n",
    "        X_train, X_test = df.iloc[train].loc[:, df.columns != 'y'], df.iloc[test].loc[:, df.columns != 'y']\n",
    "        y_train, y_test = df.iloc[train].loc[:,'y'], df.iloc[test].loc[:,'y']\n",
    "\n",
    "        X_train  = center_normalize_array(X_train)\n",
    "        \n",
    "        lin = reg.fit(X_train, y_train)\n",
    "        y_hat = reg.predict(X_test)\n",
    "\n",
    "        kf_RMSE.append(np.sqrt(((y_hat - y_test) ** 2).mean()))\n",
    "        kf_r2.append(R2_RMSE(y_test, X_test, B, y_hat)[1])\n",
    "    \n",
    "\n",
    "    kf_RMSE = (1/10) * np.sum(kf_RMSE)\n",
    "    kf_r2 = (1/10) * np.sum(kf_r2)\n",
    "    \n",
    "    return kf_RMSE, kf_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46e54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_lasso(alpha, B):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.linear_model import Lasso\n",
    "    \n",
    "    kf_RMSE = []\n",
    "    kf_r2 = []\n",
    "    \n",
    "    model = Lasso(alpha = alpha)\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "    \n",
    "    for train, test in cv.split(df):\n",
    "        X_train, X_test = df.iloc[train].loc[:, df.columns != 'y'], df.iloc[test].loc[:, df.columns != 'y']\n",
    "        y_train, y_test = df.iloc[train].loc[:,'y'], df.iloc[test].loc[:,'y']\n",
    "        \n",
    "        X_train  = center_normalize_array(X_train)\n",
    "        \n",
    "        lasso = model.fit(X_train, y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        #kf_RMSE.append(np.sqrt(((y_hat - y_test) ** 2).mean()))\n",
    "        kf_RMSE.append(R2_RMSE(y_test, X_test, B, y_hat)[0])\n",
    "        kf_r2.append(R2_RMSE(y_test, X_test, B, y_hat)[1])\n",
    "    \n",
    "\n",
    "    kf_RMSE = (1/10) * np.sum(kf_RMSE)\n",
    "    kf_r2 = (1/10) * np.sum(kf_r2)\n",
    "    \n",
    "    return kf_RMSE, kf_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e04d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Coef are\n",
      "[-2.77555756e-18  0.00000000e+00]\n",
      "\n",
      "RMSE fit is\n",
      "RMSE = 0.07071067811865475\n",
      "R2 fit is\n",
      "R2 = 0.0\n",
      "\n",
      "RMSE CV is\n",
      "RMSE = 0.07664585391933734\n",
      "R2 CV is\n",
      "R2 = -0.5654776412806074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#null model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "X_null = np.ones((df_cm.iloc[:,0].shape[0],1))\n",
    "y = df['y']\n",
    "reg = LinearRegression().fit(X_null, y)\n",
    "B_null = reg.coef_\n",
    "B_null = np.insert(B_null, 0, reg.intercept_, axis=0)\n",
    "print('The Coef are')\n",
    "print(B_null)\n",
    "\n",
    "y_hat_null = reg.predict(X_null)\n",
    "r2_rmse_null = R2_RMSE(y, X_null, B_null, y_hat_null)\n",
    "print('\\nRMSE fit is\\nRMSE = %s' % (r2_rmse_null[0]))\n",
    "print('R2 fit is\\nR2 = %s' % (r2_rmse_null[1]))\n",
    "\n",
    "r2_rmse_null_cv = k_fold_cv_lin(B_null)\n",
    "\n",
    "print('\\nRMSE CV is\\nRMSE = %s' % (r2_rmse_null_cv[0]))\n",
    "print('R2 CV is\\nR2 = %s' % (r2_rmse_null_cv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3223dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Coef are\n",
      "[-2.14102255e-18  2.26009703e-01  4.11413847e-01  6.18120610e-01\n",
      " -8.48310544e-02 -5.21649659e-03]\n",
      "\n",
      "abs B is = \n",
      "1.3455917106580257\n",
      "\n",
      "RMSE fit is\n",
      "RMSE = 0.04273550300119457\n",
      "R2 fit is\n",
      "R2 = 0.6324079530484797\n",
      "\n",
      "RMSE CV is\n",
      "RMSE = 0.07664585391933731\n",
      "R2 CV is\n",
      "R2 = -0.565477641280606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#full model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "X_full = df_cm[['x1', 'x2', 'x3', 'x4', 'x5']]\n",
    "y = df['y']\n",
    "\n",
    "reg2 = LinearRegression().fit(X_full, y)\n",
    "B_full = reg2.coef_\n",
    "B_full = np.insert(B_full, 0, reg2.intercept_, axis=0)\n",
    "print('The Coef are')\n",
    "print(B_full)\n",
    "print('\\nabs B is = ')\n",
    "print(np.sum(np.abs(reg2.coef_)))\n",
    "\n",
    "y_hat_full = reg2.predict(X_full)\n",
    "\n",
    "r2_rmse_lin = R2_RMSE(y, X_full, B_full, y_hat_full)\n",
    "print('\\nRMSE fit is\\nRMSE = %s' % (r2_rmse_lin[0]))\n",
    "print('R2 fit is\\nR2 = %s' % (r2_rmse_lin[1]))\n",
    "\n",
    "r2_rmse_lin_cv = k_fold_cv_lin(B_full)\n",
    "print('\\nRMSE CV is\\nRMSE = %s' % (r2_rmse_lin_cv[0]))\n",
    "print('R2 CV is\\nR2 = %s' % (r2_rmse_lin_cv[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e28325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Coef are\n",
      "[-1.11732459e-18  2.26009703e-01  4.11413847e-01  6.18120610e-01\n",
      " -8.48310544e-02 -5.21649659e-03]\n",
      "\n",
      "abs B is = \n",
      "1.3455917106580273\n",
      "\n",
      "lambda best is = 0.000000\n",
      "\n",
      "RMSE fit is\n",
      "RMSE = 0.04273550300119457\n",
      "R2 fit is\n",
      "R2 = 0.6324079530484799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16406342696144283, tolerance: 8.887730997296937e-05\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15370937121305506, tolerance: 9.162580411029365e-05\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1671802284400994, tolerance: 9.194089543451232e-05\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16557052914949089, tolerance: 8.992431616732854e-05\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1714: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X, y)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.826e-01, tolerance: 1.000e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.641e-01, tolerance: 8.888e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e-01, tolerance: 9.214e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e-01, tolerance: 8.395e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e-01, tolerance: 8.901e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e-01, tolerance: 9.163e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.667e-01, tolerance: 9.056e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE CV is\n",
      "RMSE = 0.07664585391933726\n",
      "R2 CV is\n",
      "R2 = -0.5654776412806031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e-01, tolerance: 9.380e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.672e-01, tolerance: 9.194e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e-01, tolerance: 8.766e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.656e-01, tolerance: 8.992e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#lasso with lasso cv\n",
    "\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "X_lasso = df_cm[['x1', 'x2', 'x3', 'x4', 'x5']]\n",
    "y = df['y']\n",
    "\n",
    "lambdas = np.linspace(0,0.1,101)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "model = LassoCV(alphas = lambdas, cv=cv)\n",
    "lassocv = model.fit(X_lasso, y)\n",
    "\n",
    "B_lasso = lassocv.coef_\n",
    "B_lasso = np.insert(B_lasso, 0, lassocv.intercept_, axis=0)\n",
    "\n",
    "print('The Coef are')\n",
    "print(B_lasso)\n",
    "print('\\nabs B is = ')\n",
    "print(np.sum(np.abs(lassocv.coef_)))\n",
    "print('\\nlambda best is = %f' % lassocv.alpha_)\n",
    "\n",
    "y_hat_lasso = lassocv.predict(X_lasso)\n",
    "\n",
    "r2_rmse_lasso = R2_RMSE(y, X_lasso, B_lasso, y_hat_lasso)\n",
    "print('\\nRMSE fit is\\nRMSE = %s' % (r2_rmse_lasso[0]))\n",
    "print('R2 fit is\\nR2 = %s' % (r2_rmse_lasso[1]))\n",
    "\n",
    "\n",
    "r2_rmse_lasso_cv = k_fold_cv_lasso(lassocv.alpha_, B_lasso)\n",
    "print('\\nRMSE CV is\\nRMSE = %s' % (r2_rmse_lasso_cv[0]))\n",
    "print('R2 CV is\\nR2 = %s' % (r2_rmse_lasso_cv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93b5ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.641e-01, tolerance: 8.888e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e-01, tolerance: 9.214e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e-01, tolerance: 8.395e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e-01, tolerance: 8.901e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e-01, tolerance: 9.163e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.667e-01, tolerance: 9.056e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e-01, tolerance: 9.380e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.672e-01, tolerance: 9.194e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e-01, tolerance: 8.766e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_6224\\2491081572.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.656e-01, tolerance: 8.992e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#Lasso with re-scaling\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "lambdas = np.linspace(0,0.1,101)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "\n",
    "RMSE_lambda_cv = []\n",
    "RMSE_lambda = []\n",
    "\n",
    "for i in lambdas:\n",
    "    model = Lasso(alpha = i)\n",
    "    \n",
    "    for train, test in cv.split(df):\n",
    "        X_train, X_test = df.iloc[train].loc[:, df.columns != 'y'], df.iloc[test].loc[:, df.columns != 'y']\n",
    "        y_train, y_test = df.iloc[train].loc[:,'y'], df.iloc[test].loc[:,'y']\n",
    "        \n",
    "        X_train  = center_normalize_array(X_train)\n",
    "        #print(X_train)\n",
    "        \n",
    "        lasso = model.fit(X_train, y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        RMSE_lambda_cv.append(np.sqrt(((y_hat - y_test) ** 2).mean()))\n",
    "    \n",
    "    RMSE_lambda.append((1/10) * np.sum(RMSE_lambda_cv))\n",
    "    RMSE_lambda_cv = []\n",
    "    \n",
    "df_lambda = pd.DataFrame({'lambdas':lambdas, 'RMSE': RMSE_lambda})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "765a21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lambdas      RMSE\n",
      "0      0.000  0.043404\n",
      "1      0.001  0.049597\n",
      "2      0.002  0.059252\n",
      "3      0.003  0.067432\n",
      "73     0.073  0.070482\n",
      "..       ...       ...\n",
      "31     0.031  0.070482\n",
      "30     0.030  0.070482\n",
      "29     0.029  0.070482\n",
      "27     0.027  0.070482\n",
      "100    0.100  0.070482\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_lambda.sort_values(by=['RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594e745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Coef are\n",
      "[-5.79745201e-19  9.66404528e-03  2.18572134e-01  4.32674031e-01\n",
      " -0.00000000e+00 -0.00000000e+00]\n",
      "\n",
      "abs B is = \n",
      "0.6609102105491518\n",
      "\n",
      "RMSE fit is\n",
      "RMSE = 0.04945804221088604\n",
      "R2 fit is\n",
      "R2 = 0.5089242325087437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE CV is\n",
      "RMSE = 0.054691067266911836\n",
      "R2 CV is\n",
      "R2 = 0.3683023512645963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#lasso with re-scaling lamba = 0.001\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "X_lasso2 = df_cm[['x1', 'x2', 'x3', 'x4', 'x5']]\n",
    "y = df['y']\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model2 = Lasso(alpha = 0.001)\n",
    "lasso2 = model2.fit(X_lasso2, y)\n",
    "\n",
    "B_lasso2 = lasso2.coef_\n",
    "B_lasso2 = np.insert(B_lasso2, 0, lasso2.intercept_, axis=0)\n",
    "\n",
    "print('The Coef are')\n",
    "print(B_lasso2)\n",
    "print('\\nabs B is = ')\n",
    "print(np.sum(np.abs(lasso2.coef_)))\n",
    "\n",
    "y_hat_lasso2 = lasso2.predict(X_lasso2)\n",
    "\n",
    "r2_rmse_lasso2 = R2_RMSE(y, X_lasso2, B_lasso2, y_hat_lasso2)\n",
    "print('\\nRMSE fit is\\nRMSE = %s' % (r2_rmse_lasso2[0]))\n",
    "print('R2 fit is\\nR2 = %s' % (r2_rmse_lasso2[1]))\n",
    "\n",
    "r2_rmse_lasso2_cv = k_fold_cv_lasso(0.001, B_lasso2)\n",
    "print('\\nRMSE CV is\\nRMSE = %s' % (r2_rmse_lasso2_cv[0]))\n",
    "print('R2 CV is\\nR2 = %s' % (r2_rmse_lasso2_cv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7fef223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.641e-01, tolerance: 8.888e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e-01, tolerance: 9.214e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e-01, tolerance: 8.395e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e-01, tolerance: 8.901e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e-01, tolerance: 9.163e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.667e-01, tolerance: 9.056e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e-01, tolerance: 9.380e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.672e-01, tolerance: 9.194e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e-01, tolerance: 8.766e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2884282680.py:23: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.656e-01, tolerance: 8.992e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#Lasso without re-scaling\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "lambdas = np.linspace(0,0.1,101)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "\n",
    "RMSE_lambda_cv = []\n",
    "RMSE_lambda = []\n",
    "\n",
    "for i in lambdas:\n",
    "    model = Lasso(alpha = i)\n",
    "    \n",
    "    for train, test in cv.split(df):\n",
    "        X_train, X_test = df.iloc[train].loc[:, df.columns != 'y'], df.iloc[test].loc[:, df.columns != 'y']\n",
    "        y_train, y_test = df.iloc[train].loc[:,'y'], df.iloc[test].loc[:,'y']\n",
    "        \n",
    "        #X_train  = center_normalize_array(X_train)\n",
    "        lasso = model.fit(X_train, y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        RMSE_lambda_cv.append(np.sqrt(((y_hat - y_test) ** 2).mean()))\n",
    "    \n",
    "    RMSE_lambda.append((1/10) * np.sum(RMSE_lambda_cv))\n",
    "    RMSE_lambda_cv = []\n",
    "    \n",
    "df_lambda = pd.DataFrame({'lambdas':lambdas, 'RMSE': RMSE_lambda})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60879522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambdas</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.049391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.060004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.068705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.070482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.029</td>\n",
       "      <td>0.070482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.070482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.070482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lambdas      RMSE\n",
       "0      0.000  0.043624\n",
       "1      0.001  0.049391\n",
       "2      0.002  0.060004\n",
       "3      0.003  0.068705\n",
       "73     0.073  0.070482\n",
       "..       ...       ...\n",
       "31     0.031  0.070482\n",
       "30     0.030  0.070482\n",
       "29     0.029  0.070482\n",
       "27     0.027  0.070482\n",
       "100    0.100  0.070482\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lambda.sort_values(by=['RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff78e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\2340728518.py:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso3 = model3.fit(X_lasso3, y)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.826e-01, tolerance: 1.000e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.641e-01, tolerance: 8.888e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e-01, tolerance: 9.214e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Coef are\n",
      "[-8.38320962e-19  2.26009703e-01  4.11413847e-01  6.18120610e-01\n",
      " -8.48310544e-02 -5.21649659e-03]\n",
      "\n",
      "abs B is = \n",
      "1.345591710658026\n",
      "\n",
      "RMSE fit is\n",
      "RMSE = 0.04273550300119457\n",
      "R2 fit is\n",
      "R2 = 0.6324079530484799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e-01, tolerance: 8.395e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e-01, tolerance: 8.901e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e-01, tolerance: 9.163e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.667e-01, tolerance: 9.056e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e-01, tolerance: 9.380e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.672e-01, tolerance: 9.194e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e-01, tolerance: 8.766e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE CV is\n",
      "RMSE = 0.07664585391933726\n",
      "R2 CV is\n",
      "R2 = -0.5654776412806034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woon\\AppData\\Local\\Temp\\ipykernel_21468\\1236838267.py:18: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso = model.fit(X_train, y_train)\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\Woon\\Documents\\env_ds_3_10\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.656e-01, tolerance: 8.992e-05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#lasso without re-scaling lamba = 0.000\n",
    "df_cm = center_normalize_df(df)\n",
    "\n",
    "X_lasso3 = df_cm[['x1', 'x2', 'x3', 'x4', 'x5']]\n",
    "y = df['y']\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model3 = Lasso(alpha = 0.000)\n",
    "lasso3 = model3.fit(X_lasso3, y)\n",
    "\n",
    "B_lasso3 = lasso3.coef_\n",
    "B_lasso3 = np.insert(B_lasso3, 0, lasso3.intercept_, axis=0)\n",
    "\n",
    "print('The Coef are')\n",
    "print(B_lasso3)\n",
    "print('\\nabs B is = ')\n",
    "print(np.sum(np.abs(lasso3.coef_)))\n",
    "\n",
    "y_hat_lasso3 = lasso3.predict(X_lasso3)\n",
    "\n",
    "r2_rmse_lasso3 = R2_RMSE(y, X_lasso3, B_lasso3, y_hat_lasso3)\n",
    "print('\\nRMSE fit is\\nRMSE = %s' % (r2_rmse_lasso3[0]))\n",
    "print('R2 fit is\\nR2 = %s' % (r2_rmse_lasso3[1]))\n",
    "\n",
    "r2_rmse_lasso3_cv = k_fold_cv_lasso(0.000, B_lasso3)\n",
    "print('\\nRMSE CV is\\nRMSE = %s' % (r2_rmse_lasso3_cv[0]))\n",
    "print('R2 CV is\\nR2 = %s' % (r2_rmse_lasso3_cv[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds_3_10",
   "language": "python",
   "name": "env_ds_3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
